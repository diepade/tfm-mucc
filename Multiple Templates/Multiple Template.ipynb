{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc6bea8e-a003-41d3-b127-53e78143e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function= {}\n",
    "Example = {}\n",
    "\n",
    "function['A'] = \"\"\"You are an extraction model designed to identify and extract Personally Identifiable Information (PII) from text and format it into a structured JSON. Your task is to analyze the provided text and extract the relevant data fields as specified below. \n",
    "\n",
    "Ensure that the output adheres to the exact JSON structure, including all fields, even if they are empty. If no data is found, output an empty JSON with the same structure.\n",
    "\n",
    "\"\"\"\n",
    "function['B'] = \"\"\"\n",
    "Please provide only the JSON output, with no additional comments or explanations. Use the following template as base:\n",
    "\"\"\"\n",
    "\n",
    "function['Person'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "- \"Name\": Person's full name.\n",
    "- \"Birth_Date\": Date of birth.\n",
    "- \"Age\": Age of the person.\n",
    "- \"Height\": Person's height.\n",
    "- \"Weight\": Person's weight.\n",
    "- \"Gender\": Person's gender.\n",
    "- \"Marital_Status\": Person's marital status.\n",
    "- \"Number_of_Children\": Number of children.\n",
    "- \"Place_of_Birth\": Birthplace of the person.\n",
    "- \"Mother_Maiden_Name\": Mother’s maiden name.\n",
    "- \"Sexual_Preference\": Sexual preference.\n",
    "- \"Sex_Life\": Sex life details.\n",
    "\"\"\"\n",
    "Example['Person'] = \"\"\"\n",
    "{\n",
    "    \"Name\": \"\",\n",
    "    \"Birth_Date\": \"\",\n",
    "    \"Age\": \"\",\n",
    "    \"Height\": \"\",\n",
    "    \"Weight\": \"\",\n",
    "    \"Gender\": \"\",\n",
    "    \"Marital_Status\": \"\",\n",
    "    \"Number_of_Children\": \"\",\n",
    "    \"Place_of_Birth\": \"\",\n",
    "    \"Mother_Maiden_Name\": \"\",\n",
    "    \"Sexual_Preference\": \"\",\n",
    "    \"Sex_Life\": \"\"\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "function['Location'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "- \"Home_Town_City\": Person's home town.\n",
    "- \"Geographical_Indicators\": Geographical Indicators of a Person location.\n",
    "- \"Geo_Location\": Coordinates of a Person location.\n",
    "- \"Country\": Country where a Person is currently in.\n",
    "- \"ZIP_Code\": ZIP Code of a Person Location.\n",
    "- \"Address\": Current Address of a Person Location.\n",
    "- \"Date_Time\": Date and Time Info on a Person Routine.\n",
    "\"\"\"\n",
    "Example['Location'] = \"\"\"\n",
    "{\n",
    "    \"Home_Town_City\": \"\",\n",
    "    \"Geographical_Indicators\": \"\",\n",
    "    \"Geo_Location\": \"\",\n",
    "    \"Country\": \"\",\n",
    "    \"ZIP_Code\": \"\",\n",
    "    \"Address\": \"\",\n",
    "    \"Date_Time\": \"\"\n",
    "}\"\"\"\n",
    "\n",
    "function['Contact'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "    - \"Home_Address\": A list containing: \n",
    "        - \"Street_Address\": A Street name of a Person's home,\n",
    "        - \"City\": The City of a Person's Home.\n",
    "        - \"State\":The State of a Person's Home.\n",
    "        - \"ZIP_Code\": The ZIP Code of a Person's Home.\n",
    "        - \"Country\": The Country where a Person's Home is. \n",
    "    - \"Phone_Number\": A Personal Phone Number.\n",
    "    - \"Email_Address\": A Person Email Address\n",
    "    - \"Family_Friend_Contact_Information\": A Person Related Contact Info.\n",
    "\"\"\"\n",
    "Example['Contact'] = \"\"\"\n",
    "{ \n",
    "    \"Home_Address\": { \n",
    "        \"Street_Address\": \"\",\n",
    "        \"City\": \"\",\n",
    "        \"State\": \"\",\n",
    "        \"ZIP_Code\": \"\",\n",
    "        \"Country\": \"\" \n",
    "    },\n",
    "    \"Phone_Number\": \"\",\n",
    "    \"Email_Address\": \"\",\n",
    "    \"Family_Friend_Contact_Information\": \"\"\n",
    "}\"\"\"\n",
    "\n",
    "function['Identifiers'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "    - \"Personal_Identifiers\": A list containing:\n",
    "        - \"National_ID\": A Person National Identity Document \n",
    "        - \"Passport_Number\": A Person Passport Number\n",
    "        - \"Social_Security_Number\": A Person Social Security Number\n",
    "        - \"Vehicle_Registration\": A Person's Vehicle Registration Number\n",
    "    - \"Online_Identifiers\": A list containing:\n",
    "        - \"Screen_Name\": A Person casual screen name.\n",
    "        - \"Social_Network_Profile\": A Person Profile in Social Networks.\n",
    "        - \"Social_Network_Activity\": A Person usual activity online.\n",
    "        - \"URLs\": A Person related URL.\n",
    "        - \"Online_Aliases\": A Person related online Aliases.\n",
    "        - \"IPs\": A Person related IP address.   \n",
    "\"\"\"\n",
    "\n",
    "Example['Identifiers'] = \"\"\"\n",
    "{\n",
    "    \"Personal_Identifiers\":{\n",
    "        \"National_ID\": \"\", \n",
    "        \"Passport_Number\": \"\", \n",
    "        \"Social_Security_Number\": \"\", \n",
    "        \"Vehicle_Registration\": \"\" \n",
    "    }, \n",
    "    \"Online_Identifiers\": { \n",
    "        \"Screen_Name\": \"\", \n",
    "        \"Social_Network_Profile\": \"\", \n",
    "        \"Social_Network_Activity\": \"\", \n",
    "        \"URLs\": \"\", \n",
    "        \"Online_Aliases\": \"\", \n",
    "        \"IPs\": \"\" \n",
    "    }\n",
    "}\"\"\"\n",
    "\n",
    "function['NRP'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "    - \"Nationality_Citizenship\": A Person Nationality or Citizenship.\n",
    "    - \"Race_Ethnic\": A Person Race or Ethnicity.\n",
    "    - \"Religion\": A Person Religion.\n",
    "    - \"Philosophical_Belief\": A Person Philosophical Belief.\n",
    "    - \"Political_Affiliation\": A Person Political Affiliation.\n",
    "    - \"Trade_Union_Affiliation\": A Person Trade Union Affiliation. \n",
    "\"\"\"\n",
    "\n",
    "Example['NRP'] = \"\"\"\n",
    "{\n",
    "    \"Nationality_Citizenship\": \"\",\n",
    "    \"Race_Ethnic\": \"\",\n",
    "    \"Religion\": \"\",\n",
    "    \"Philosophical_Belief\": \"\",\n",
    "    \"Political_Affiliation\": \"\",\n",
    "    \"Trade_Union_Affiliation\": \"\"\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "function['Finance'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "    - \"Banking_Details\": A list containing:\n",
    "\t\t- \"Credit_Card_Number\": A Person Credit Card Number\n",
    "\t\t- \"Credit_Score\": A Person given credit score\n",
    "\t\t- \"ABA_Routing_Number\": A Person ABA Routing Number\n",
    "\t\t- \"Bank_Account_Number\": A Person Bank Account Number\n",
    "\t\t- \"Individual_Taxpayer_Identification\": A Person Taxpayer Identifier\n",
    "\t\t- \"SWIFT_Code\": A Person SWIFT Code\n",
    "\t\t- \"Crypto\": A Person Crypto Wallet\n",
    "\t- \"Invoice_Payments\": A Person Invoice Payment Info\n",
    "\t- \"Financial_Information\": Any other Financial Related Info.\n",
    "\"\"\"\n",
    "\n",
    "Example['Finance'] = \"\"\"\n",
    "{\n",
    "    \"Banking_Details\": {\n",
    "\t\t\"Credit_Card_Number\": \"\",\n",
    "\t\t\"Credit_Score\": \"\",\n",
    "\t\t\"ABA_Routing_Number\": \"\",\n",
    "\t\t\"Bank_Account_Number\": \"\",\n",
    "\t\t\"Individual_Taxpayer_Identification\": \"\",\n",
    "\t\t\"SWIFT_Code\": \"\",\n",
    "\t\t\"Crypto\": \"\"\n",
    "\t},\n",
    "\t\"Invoice_Payments\": \"\",\n",
    "\t\"Financial_Information\": \"\"\n",
    "}\"\"\"\n",
    "\n",
    "function['Security'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "\t- \"Digital_Signature\": The HASH Footprint of a Person Digital Signature\n",
    "\t- \"Password\": A Person related Password for any kind of activity.\n",
    "\t- \"License_Numbers\": A list containing:\n",
    "\t\t- \"Drivers_License_Number\": A Person Driver's Licence Number\n",
    "\t\t- \"Vehicle_Registration_Number\": A Person's Vehicle Registration Number\n",
    "\t\t- \"License_Plate_Number\": A Licence Plate Number related to a Person.\n",
    "\t- \"Biometric_Data\": A list containing:\n",
    "\t\t- \"Fingerprint_Data\": The biometric information about a Person Fingerprint.\n",
    "\t\t- \"Voice_Print\": The Characteristic Data about a Person Voice Print.\n",
    "\t\t- \"Handwriting_Sample\": A Sample of a Person Handwrite.\n",
    "\t\t- \"Physiological_Data\": Any Phisiological Data related to a Person.\n",
    "\t\t- \"Genetic_Data\": Any Genetic Data related to a Person.\n",
    "\t\t- \"X_Ray\": Any X Ray log on a Person.\n",
    "\"\"\"\n",
    "\n",
    "Example['Security'] = \"\"\"\n",
    "{\n",
    "\t\"Digital_Signature\": \"\",\n",
    "\t\"Password\": \"\",\n",
    "\t\"License_Numbers\": {\n",
    "\t\t\"Drivers_License_Number\": \"\",\n",
    "\t\t\"Vehicle_Registration_Number\": \"\",\n",
    "\t\t\"License_Plate_Number\": \"\"\n",
    "\t},\n",
    "\t\"Biometric_Data\": {\n",
    "\t\t\"Fingerprint_Data\": \"\",\n",
    "\t\t\"Voice_Print\": \"\",\n",
    "\t\t\"Handwriting_Sample\": \"\",\n",
    "\t\t\"Physiological_Data\": \"\",\n",
    "\t\t\"Genetic_Data\": \"\",\n",
    "\t\t\"X_Ray\": \"\"\n",
    "\t}\n",
    "}\"\"\"\n",
    "\n",
    "function['Work'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "\t- \"Job_Title\": A Person Job Title\n",
    "\t- \"Occupation\": A Person Life Occupation\n",
    "\t- \"Work_ID\": A Person Work related ID\n",
    "\t- \"Work_Address\": A list containing:\n",
    "\t\t- \"Street_Address\": The Address of a Person workplace\n",
    "\t\t- \"City\": The City of a Person workplace\n",
    "\t\t- \"State\": The State where a Person workplace is.\n",
    "\t\t- \"ZIP_Code\": The ZIP Code of a Person workplace.\n",
    "\t\t- \"Country\": The Contry where a Person workplace is.\n",
    "\t- \"Work_Contact_Information\": A list containing:\n",
    "\t\t- \"Work_Phone_Number\": A Person Work Phone Number.\n",
    "\t\t- \"Work_Email_Address\": A Person Work Email Address.\n",
    "\t- \"Employment_Information\": A list containing:\n",
    "\t\t- \"Employment_Status\": The info on the Employment Status of a Person.\n",
    "\t\t- \"Work_Experience\": Info related to Work Experience.\n",
    "\t\t- \"Skills\": The said skills of a Person related to a Job.\n",
    "\t\t- \"Education\": The Educational Level of a Person.\n",
    "    - \"Income_Level\": Info related to a Person Income Level.\n",
    "\"\"\"\n",
    "\n",
    "Example['Work'] = \"\"\"\n",
    "{\n",
    "\t\"Job_Title\": \"\",\n",
    "\t\"Occupation\": \"\",\n",
    "\t\"Work_ID\": \"\",\n",
    "\t\"Work_Address\": {\n",
    "\t\t\"Street_Address\": \"\",\n",
    "\t\t\"City\": \"\",\n",
    "\t\t\"State\": \"\",\n",
    "\t\t\"ZIP_Code\": \"\",\n",
    "\t\t\"Country\": \"\"\n",
    "\t},\n",
    "\t\"Work_Contact_Information\": {\n",
    "\t\t\"Work_Phone_Number\": \"\",\n",
    "\t\t\"Work_Email_Address\": \"\"\n",
    "\t},\n",
    "\t\"Employment_Information\": {\n",
    "\t\t\"Employment_Status\": \"\",\n",
    "\t\t\"Work_Experience\": \"\",\n",
    "\t\t\"Skills\": \"\",\n",
    "\t\t\"Education\": \"\"\n",
    "    },\n",
    "    \"Income_Level\": \"\"\n",
    "}\"\"\"\n",
    "\n",
    "function['Others'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "    - \"Health_Information\": A list containing:\n",
    "\t\t- \"Health_Insurance_ID\": The Health Insurance ID Number of a Person.\n",
    "\t\t- \"Medical_History\": Info related to the Medical History of a Person.\n",
    "\t\t- \"Physiological_Data\": Info related to Phisiological Data of a Person.\n",
    "\t- \"Cultural_and_Social_Identity\": A list containing:\n",
    "\t\t- \"Cultural_Social_Identity\": A Person Socio-cultural Identity.\n",
    "\t\t- \"Shopping_Behavior\": Info related to Shopping Behavior of a Person.\n",
    "\t\t- \"Survey_Answers\": Any Survey related answers related to a Person.\n",
    "\t\t- \"Signed_Petitions\": Any Signed Petitions Info related to a Person.\n",
    "\t\t- \"Activities\": Any extra activities related to a Person.\n",
    "\t\t- \"Law_Enforcement_Records\": Any Law Enforcing Records related to a Person.\n",
    "    - \"Appearance\": A list containing:\n",
    "\t\t- \"Picture_of_Face\": The Bitcode related to a Person's Facial Footprint or a Description of a Person Face.\n",
    "\t\t- \"Distinguishing_Characteristic\": A Person full-body recognizable characteristics.\n",
    "\"\"\"\n",
    "\n",
    "Example['Others'] = \"\"\"\n",
    "{\n",
    "    \"Health_Information\": {\n",
    "\t\t\"Health_Insurance_ID\": \"\",\n",
    "\t\t\"Medical_History\": \"\",\n",
    "\t\t\"Physiological_Data\": \"\"\n",
    "\t},\n",
    "\t\"Cultural_and_Social_Identity\": {\n",
    "\t\t\"Cultural_Social_Identity\": \"\",\n",
    "\t\t\"Shopping_Behavior\": \"\",\n",
    "\t\t\"Survey_Answers\": \"\",\n",
    "\t\t\"Signed_Petitions\": \"\",\n",
    "\t\t\"Activities\": \"\",\n",
    "\t\t\"Law_Enforcement_Records\": \"\"\n",
    "\t},\n",
    "    \"Appearance\": {\n",
    "\t\t\"Picture_of_Face\": \"\",\n",
    "\t\t\"Distinguishing_Characteristic\": \"\"\n",
    "\t}\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "eqi_table = {}\n",
    "\n",
    "eqi_table['Person'] = ['LOCATION','PERSON']\n",
    "eqi_table['Location'] = ['DATE_TIME','LOCATION']\n",
    "eqi_table['Contact'] = ['LOCATION','PHONE_NUMBER']\n",
    "eqi_table['Identifiers'] = ['IP_ADDRESS','URL','US_PASSPORT','US_SSN','UK_NINO','ES_NIF','ES_NIE','IT_FISCAL_CODE','IT_PASSPORT','IT_IDENTITY_CARD','PL_PESEL','SG_NRIC_FIN','SG_UEN','AU_ABN','AU_ACN','IN_PAN','IN_AADHAAR','IN_VOTER','IN_PASSPORT','FI_PERSONAL_IDENTITY_CODE','ID']\n",
    "eqi_table['NRP'] = ['NRP','IN_VOTER']\n",
    "eqi_table['Finance'] = ['CREDIT_CARD','CRYPTO','IBAN_CODE','US_BANK_NUMBER','US_ITIN','UK_NINO','IT_FISCAL_CODE','IT_VAT_CODE','AU_TFN']\n",
    "eqi_table['Security'] = ['IT_DRIVER_LICENSE','US_DRIVER_LICENSE','IN_VEHICLE_REGISTRATION']\n",
    "eqi_table['Work'] = ['DATE_TIME','LOCATION','MEDICAL_LICENSE','AU_ABN']\n",
    "eqi_table['Others'] = ['MEDICAL_LICENSE','UK_NHS','UK_NINO','AU_MEDICARE']\n",
    "\n",
    "table_eqi = {}\n",
    "table_eqi['PERSON'] = {'Person':['Name',\n",
    "                                 'Mother_Maiden_Name'],\n",
    "                       'Contact':['Family_Friend_Contact_Info']\n",
    "                      }\n",
    "\n",
    "\n",
    "table_eqi['LOCATION'] = {'Person':['Place_of_Birth'],\n",
    "                         'Location': ['Home_Town_City',\n",
    "                                      'Country',\n",
    "                                      'Address'],\n",
    "                         'Contact':{'Home_Address': ['Street_Address',\n",
    "                                                     'City',\n",
    "                                                     'State',\n",
    "                                                     'Country']\n",
    "                                   }\n",
    "                        }\n",
    "\n",
    "table_eqi['EMAIL'] = {'Contact':['Email_Address'],\n",
    "                          'Work': {'Work_Contact_Info':['Work_Email_Address']\n",
    "                                  }\n",
    "                     }\n",
    "\n",
    "table_eqi['DATE_TIME'] = {'Person':['Birth_Date'],\n",
    "                          'Location':['Date_Time']\n",
    "                         }\n",
    "\n",
    "table_eqi['NRP'] = {'NRP':['Nationality_Citizenship',\n",
    "                           'Race_Ethnic',\n",
    "                           'Religion',\n",
    "                           'Philosophical_Belief',\n",
    "                           'Political_Affiliation',\n",
    "                           'Trade_Union_Affiliation']\n",
    "                   }\n",
    "\n",
    "table_eqi['PASSPORT'] = {'Identifiers':{'Personal_Identifiers': ['Passport_Number']\n",
    "                                       },\n",
    "                        }\n",
    "\n",
    "table_eqi['PHONE_NUMBER'] = {'Contact':['Phone_Number'],\n",
    "                             'Work':{'Work_Contact_Information':['Work_Phone_Number']\n",
    "                                    }\n",
    "                            }\n",
    "\n",
    "table_eqi['URL'] = {'Identifiers':{'Online_Identifiers': ['URLs']\n",
    "                                       }\n",
    "                        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7eb4799-323f-48a9-baeb-9430591b23fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "function= {}\n",
    "Example = {}\n",
    "\n",
    "function['A'] = \"\"\"You are an extraction model designed to identify and extract Personally Identifiable Information (PII) from text and format it into a structured JSON. Your task is to analyze the provided text and extract the relevant data fields as specified below. \n",
    "\n",
    "Ensure that the output adheres to the exact JSON structure, including all fields, even if they are empty. If data for a field is not found, output it as \"\".\n",
    "\n",
    "\"\"\"\n",
    "function['B'] = \"\"\"\n",
    "Please provide only the JSON output, with no additional comments or explanations. Do not create any extra data or try to fill non-found data. TThe output MUST be clear Use the following template as base:\n",
    "\"\"\"\n",
    "\n",
    "function['Template'] = \"\"\"\n",
    "The required JSON fields are:\n",
    "    - Personal_Information: A list containing:\n",
    "        - \"Person\": A list containing:\n",
    "            - \"Name\": Person's full name.\n",
    "            - \"National_ID\": A Person National Identity Document \n",
    "            - \"Passport_Number\": A Person Passport Number\n",
    "            - \"Social_Security_Number\": A Person Social Security Number\n",
    "            - \"Birth_Date\": Date of birth.\n",
    "            - \"Age\": Age of the person.\n",
    "            - \"Height\": Person's height.\n",
    "            - \"Weight\": Person's weight.\n",
    "            - \"Gender\": Person's gender.\n",
    "            - \"Marital_Status\": Person's marital status.\n",
    "            - \"Number_of_Children\": Number of children.\n",
    "            - \"Nationality_Citizenship\": A Person Nationality or Citizenship.\n",
    "            - \"Place_of_Birth\": Birthplace of the person.\n",
    "            - \"Mother_Maiden_Name\": Mother’s maiden name.\n",
    "            - \"Race_Ethnic\": A Person Race or Ethnicity.\n",
    "            - \"Religion\": A Person Religion.\n",
    "            - \"Philosophical_Belief\": A Person Philosophical Belief.\n",
    "            - \"Political_Affiliation\": A Person Political Affiliation.\n",
    "            - \"Trade_Union_Affiliation\": A Person Trade Union Affiliation. \n",
    "            - \"Sexual_Preference\": Sexual preference.\n",
    "            - \"Sex_Life\": Sex life details.\n",
    "        - \"Appearance\": A list containing:\n",
    "    \t\t- \"Picture_of_Face\": The Bitcode related to a Person's Facial Footprint or a Description of a Person Face.\n",
    "    \t\t- \"Distinguishing_Characteristic\": A Person full-body recognizable characteristics.\n",
    "        - \"Contact_Information\": A list containing:\n",
    "            - \"Home_Address\": A list containing: \n",
    "                - \"Street_Address\": A Street name of a Person's home,\n",
    "                - \"City\": The City of a Person's Home.\n",
    "                - \"State\":The State of a Person's Home.\n",
    "                - \"ZIP_Code\": The ZIP Code of a Person's Home.\n",
    "                - \"Country\": The Country where a Person's Home is. \n",
    "            - \"Phone_Number\": A Personal Phone Number.\n",
    "            - \"Email_Address\": A Person Email Address\n",
    "            - \"Family_Friend_Contact_Information\": A Person Related Contact Info.\n",
    "        - \"Online_Identifiers\": A list containing:\n",
    "            - \"Screen_Name\": A Person casual screen name.\n",
    "            - \"Social_Network_Profile\": A Person Profile in Social Networks.\n",
    "            - \"Social_Network_Activity\": A Person usual activity online.\n",
    "            - \"URLs\": A Person related URL.\n",
    "            - \"Online_Aliases\": A Person related online Aliases.\n",
    "            - \"IPs\": A Person related IP address.   \n",
    "        - \"Location_Information\": A list containing:\n",
    "            - \"Home_Town_City\": Person's home town.\n",
    "            - \"Geographical_Indicators\": Geographical Indicators of a Person location.\n",
    "            - \"Geo_Location\": Coordinates of a Person location.\n",
    "            - \"Country\": Country where a Person is currently in.\n",
    "            - \"ZIP_Code\": ZIP Code of a Person Location.\n",
    "            - \"Address\": Current Address of a Person Location.\n",
    "            - \"Date_Time\": Date and Time Info on a Person Routine.\n",
    "    - \"Work_Information\": A list containing:\n",
    "        - \"Job_Title\": A Person Job Title\n",
    "    \t- \"Occupation\": A Person Life Occupation\n",
    "    \t- \"Work_ID\": A Person Work related ID\n",
    "    \t- \"Work_Address\": A list containing:\n",
    "    \t\t- \"Street_Address\": The Address of a Person workplace\n",
    "    \t\t- \"City\": The City of a Person workplace\n",
    "    \t\t- \"State\": The State where a Person workplace is.\n",
    "    \t\t- \"ZIP_Code\": The ZIP Code of a Person workplace.\n",
    "    \t\t- \"Country\": The Contry where a Person workplace is.\n",
    "    \t- \"Work_Contact_Information\": A list containing:\n",
    "    \t\t- \"Work_Phone_Number\": A Person Work Phone Number.\n",
    "    \t\t- \"Work_Email_Address\": A Person Work Email Address.\n",
    "    \t- \"Employment_Information\": A list containing:\n",
    "    \t\t- \"Employment_Status\": The info on the Employment Status of a Person.\n",
    "    \t\t- \"Work_Experience\": Info related to Work Experience.\n",
    "    \t\t- \"Skills\": The said skills of a Person related to a Job.\n",
    "    \t\t- \"Education\": The Educational Level of a Person.\n",
    "        - \"Income_Level\": Info related to a Person Income Level.\n",
    "    - \"Financial_Information\": A list containing:\n",
    "        - \"Banking_Details\": A list containing:\n",
    "    \t\t- \"Credit_Card_Number\": A Person Credit Card Number\n",
    "    \t\t- \"Credit_Score\": A Person given credit score\n",
    "    \t\t- \"ABA_Routing_Number\": A Person ABA Routing Number\n",
    "    \t\t- \"Bank_Account_Number\": A Person Bank Account Number\n",
    "    \t\t- \"Individual_Taxpayer_Identification\": A Person Taxpayer Identifier\n",
    "    \t\t- \"SWIFT_Code\": A Person SWIFT Code\n",
    "    \t\t- \"Crypto\": A Person Crypto Wallet\n",
    "    \t- \"Invoice_Payments\": A Person Invoice Payment Info\n",
    "    \t- \"Financial_Information\": Any other Financial Related Info.\n",
    "    - \"Security_Information\": A list containing:\n",
    "        - \"Digital_Signature\": The HASH Footprint of a Person Digital Signature\n",
    "    \t- \"Password\": A Person related Password for any kind of activity.\n",
    "    \t- \"License_Numbers\": A list containing:\n",
    "    \t\t- \"Drivers_License_Number\": A Person Driver's Licence Number\n",
    "    \t\t- \"Vehicle_Registration_Number\": A Person's Vehicle Registration Number\n",
    "    \t\t- \"License_Plate_Number\": A Licence Plate Number related to a Person.\n",
    "    \t- \"Biometric_Data\": A list containing:\n",
    "    \t\t- \"Fingerprint_Data\": The biometric information about a Person Fingerprint.\n",
    "    \t\t- \"Voice_Print\": The Characteristic Data about a Person Voice Print.\n",
    "    \t\t- \"Handwriting_Sample\": A Sample of a Person Handwrite.\n",
    "    \t\t- \"Physiological_Data\": Any Phisiological Data related to a Person.\n",
    "    \t\t- \"Genetic_Data\": Any Genetic Data related to a Person.\n",
    "    \t\t- \"X_Ray\": Any X Ray log on a Person.\n",
    "        - \"Health_Information\": A list containing:\n",
    "    \t\t- \"Health_Insurance_ID\": The Health Insurance ID Number of a Person.\n",
    "    \t\t- \"Medical_History\": Info related to the Medical History of a Person.\n",
    "    \t\t- \"Physiological_Data\": Info related to Phisiological Data of a Person.\n",
    "    \t- \"Cultural_and_Social_Identity\": A list containing:\n",
    "    \t\t- \"Cultural_Social_Identity\": A Person Socio-cultural Identity.\n",
    "    \t\t- \"Shopping_Behavior\": Info related to Shopping Behavior of a Person.\n",
    "    \t\t- \"Survey_Answers\": Any Survey related answers related to a Person.\n",
    "    \t\t- \"Signed_Petitions\": Any Signed Petitions Info related to a Person.\n",
    "    \t\t- \"Activities\": Any extra activities related to a Person.\n",
    "    \t\t- \"Law_Enforcement_Records\": Any Law Enforcing Records related to a Person.\n",
    "\"\"\"\n",
    "Example['Template'] = \"\"\"\n",
    "{\n",
    "\t\"Personal_Information\": {\n",
    "\t\t\"Person\": {\n",
    "\t\t\t\"Name\": \"\",\n",
    "\t\t\t\"National_ID\": \"\",\n",
    "\t\t\t\"Passport_Number\": \"\",\n",
    "\t\t\t\"Social_Security_Number\": \"\",\n",
    "\t\t\t\"Birth_Date\": \"\",\n",
    "\t\t\t\"Age\": \"\",\n",
    "\t\t\t\"Height\": \"\",\n",
    "\t\t\t\"Weight\": \"\",\n",
    "\t\t\t\"Gender\": \"\",\n",
    "\t\t\t\"Marital_Status\": \"\",\n",
    "\t\t\t\"Number_of_Children\": \"\",\n",
    "\t\t\t\"Nationality_Citizenship\": \"\",\n",
    "\t\t\t\"Place_of_Birth\": \"\",\n",
    "\t\t\t\"Mother's_Maiden_Name\": \"\",\n",
    "\t\t\t\"Race_Ethnic\": \"\",\n",
    "\t\t\t\"Religion\": \"\",\n",
    "\t\t\t\"Philosophical_Belief\": \"\",\n",
    "\t\t\t\"Political_Affiliation\": \"\",\n",
    "\t\t\t\"Trade_Union_Affiliation\": \"\",\n",
    "\t\t\t\"Sexual_Preference\": \"\",\n",
    "\t\t\t\"Sex_Life\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Appearance\": {\n",
    "\t\t\t\"Picture_of_Face\": \"\",\n",
    "\t\t\t\"Distinguishing_Characteristic\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Contact_Information\": {\n",
    "\t\t\t\"Home_Address\": {\n",
    "\t\t\t\t\"Street_Address\": \"\",\n",
    "\t\t\t\t\"City\": \"\",\n",
    "\t\t\t\t\"State\": \"\",\n",
    "\t\t\t\t\"ZIP_Code\": \"\",\n",
    "\t\t\t\t\"Country\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"Phone_Number\": \"\",\n",
    "\t\t\t\"Email_Address\": \"\",\n",
    "\t\t\t\"Family_Friend_Contact_Information\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Online_Identifiers\": {\n",
    "\t\t\t\"Screen_Name\": \"\",\n",
    "\t\t\t\"Social_Network_Profile\": \"\",\n",
    "\t\t\t\"Social_Network_Activity\": \"\",\n",
    "\t\t\t\"URLs\": \"\",\n",
    "\t\t\t\"IPs\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Location_Information\": {\n",
    "\t\t\t\"Home_Town_City\": \"\",\n",
    "\t\t\t\"Geographical_Indicators\": \"\",\n",
    "\t\t\t\"Geo_Location\": \"\",\n",
    "\t\t\t\"Country\": \"\",\n",
    "\t\t\t\"ZIP_Code\": \"\",\n",
    "\t\t\t\"Address\": \"\",\n",
    "\t\t\t\"Date_Time\": \"\"\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"Work_Information\": {\n",
    "\t\t\"Job_Title\": \"\",\n",
    "\t\t\"Occupation\": \"\",\n",
    "\t\t\"Work_ID\": \"\",\n",
    "\t\t\"Work_Address\": {\n",
    "\t\t\t\"Street_Address\": \"\",\n",
    "\t\t\t\"City\": \"\",\n",
    "\t\t\t\"State\": \"\",\n",
    "\t\t\t\"ZIP_Code\": \"\",\n",
    "\t\t\t\"Country\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Work_Contact_Information\": {\n",
    "\t\t\t\"Work_Phone_Number\": \"\",\n",
    "\t\t\t\"Work_Email_Address\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Employment_Information\": {\n",
    "\t\t\t\"Employment_Status\": \"\",\n",
    "\t\t\t\"Work_Experience\": \"\",\n",
    "\t\t\t\"Skills\": \"\",\n",
    "\t\t\t\"Education\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Income_Level\": \"\"\n",
    "\t},\n",
    "\t\"Financial_Information\": {\n",
    "\t\t\"Banking_Details\": {\n",
    "\t\t\t\"Credit_Card_Number\": \"\",\n",
    "\t\t\t\"Credit_Score\": \"\",\n",
    "\t\t\t\"ABA_Routing_Number\": \"\",\n",
    "\t\t\t\"Bank_Account_Number\": \"\",\n",
    "\t\t\t\"Individual_Taxpayer_Identification\": \"\",\n",
    "\t\t\t\"SWIFT_Code\": \"\",\n",
    "\t\t\t\"Crypto\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Invoice_Payments\": \"\"\n",
    "\t},\n",
    "\t\"Security_Information\": {\n",
    "\t\t\"Digital_Signature\": \"\",\n",
    "\t\t\"Password\": \"\",\n",
    "\t\t\"License_Numbers\": {\n",
    "\t\t\t\"Drivers_License_Number\": \"\",\n",
    "\t\t\t\"Vehicle_Registration_Number\": \"\",\n",
    "            \"License_Plate_Number\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Biometric_Data\": {\n",
    "\t\t\t\"Fingerprint_Data\": \"\",\n",
    "\t\t\t\"Voice_Print\": \"\",\n",
    "\t\t\t\"Handwriting_Sample\": \"\",\n",
    "\t\t\t\"Physiological_Data\": \"\",\n",
    "\t\t\t\"Genetic_Data\": \"\",\n",
    "\t\t\t\"X_Ray\": \"\"\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"Health_Information\": {\n",
    "\t\t\"Health_Insurance_ID\": \"\",\n",
    "\t\t\"Medical_History\": \"\",\n",
    "\t\t\"Physiological_Data\": \"\"\n",
    "\t},\n",
    "\t\"Cultural_and_Social_Identity\": {\n",
    "\t\t\"Cultural_Social_Identity\": \"\",\n",
    "\t\t\"Shopping_Behavior\": \"\",\n",
    "\t\t\"Survey_Answers\": \"\",\n",
    "\t\t\"Signed_Petitions\": \"\",\n",
    "\t\t\"Activities\": \"\",\n",
    "\t\t\"Law_Enforcement_Records\": \"\"\n",
    "\t}\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5ea459-2221-4d41-bf82-1d072bf7fa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jZOf9E5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "label_name = \"./ShareGPT sample (N=200) - Sheet1.csv\"\n",
    "save_file = 'test.json'\n",
    "\n",
    "dataset_name = \"./sg_90k_part1.json\"\n",
    "dataset_name2 = \"./sg_90k_part2.json\"\n",
    "# Open and read the JSON file\n",
    "with open(dataset_name, 'r') as file:\n",
    "    data_1 = json.load(file)\n",
    "\n",
    "with open(dataset_name2, 'r') as file:\n",
    "    data_2 = json.load(file)\n",
    "\n",
    "data_1 = data_1 + data_2\n",
    "data_2 = []\n",
    "\n",
    "IDs = []\n",
    "PIIs = []\n",
    "with open(label_name, newline='') as csvfile:\n",
    "    spamreader = csv.DictReader(csvfile)\n",
    "    for row in spamreader:\n",
    "            IDs.append(row['Chat ID'])\n",
    "            PIIs.append(row['PII types'])\n",
    "ii = 0\n",
    "Valid_Data = []\n",
    "dataset_classes = []\n",
    "for dat in data_1:\n",
    "    k = [0]* 8\n",
    "    if dat['id'] not in IDs:\n",
    "        ii = ii+1\n",
    "        continue\n",
    "    idx = IDs.index(dat['id'])\n",
    "    PII = PIIs[idx]\n",
    "    if 'PASSPORT_NUMBER' in PII:\n",
    "        print(dat['id'])\n",
    "    k[0] = 1 if 'DATE_TIME' in PII else 0\n",
    "    k[1] = 1 if 'EMAIL_ADDRESS' in PII else 0\n",
    "    k[2] = 1 if 'LOCATION' in PII else 0\n",
    "    k[3] = 1 if 'NRP' in PII else 0\n",
    "    k[4] = 1 if 'PASSPORT_NUMBER' in PII else 0\n",
    "    k[5] = 1 if 'PERSON' in PII else 0\n",
    "    k[6] = 1 if 'PHONE_NUMBER' in PII else 0\n",
    "    k[7] = 1 if 'URL' in PII else 0\n",
    "    #print(f\"{k} - {idx} - {dat['id']} - {IDs[idx]}\")\n",
    "    Valid_Data.append({'id':IDs[idx],\n",
    "                       'PII':k,\n",
    "                       'conversations':dat['conversations']})\n",
    "\n",
    "data_1 = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dea68d7-7699-4c02-bc68-7d97c16208e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, Pattern, PatternRecognizer, RecognizerRegistry\n",
    "ID_pattern = Pattern(name=\"ID\", regex='\\\\w{0,1} {0,1}\\\\d{8} {0,1}-{0,1} {0,1}\\\\w{0,1}', score=0.7)\n",
    "ID_recognizer = PatternRecognizer(\n",
    "    supported_entity=\"ID\", patterns=[ID_pattern]\n",
    ")\n",
    "Cards_pattern = Pattern(name = 'CC', regex = '\\\\b((4\\\\d{3}|5[1-5]\\\\d{2}|2\\\\d{3}|3[47]\\\\d{1,2})[\\\\s\\\\-]?\\\\d{4,6}[\\\\s\\\\-]?\\\\d{4,6}?([\\\\s\\\\-]\\\\d{3,4})?(\\\\d{3})?)\\\\b', score=0.7)\n",
    "Cards_recognizer = PatternRecognizer(\n",
    "    supported_entity=\"CC\", patterns=[Cards_pattern]\n",
    ")\n",
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers()\n",
    "registry.add_recognizer(ID_recognizer)\n",
    "registry.add_recognizer(Cards_recognizer)\n",
    "\n",
    "\n",
    "analyzer = AnalyzerEngine(registry=registry,default_score_threshold = 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5289a-7204-4fc1-8ca8-07b438496b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8530a93f-8f52-4570-9694-e848e065eb85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-02 09:35:55 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 07-02 09:36:03 [config.py:585] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-02 09:36:03 [config.py:1697] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-02 09:36:05 [core.py:54] Initializing a V1 LLM engine (v0.8.2) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-02 09:36:06,546 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-02 09:36:06 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14710644f1a0>\n",
      "INFO 07-02 09:36:08 [parallel_state.py:954] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 07-02 09:36:08 [cuda.py:220] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-02 09:36:08 [gpu_model_runner.py:1174] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 07-02 09:36:08 [topk_topp_sampler.py:38] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "INFO 07-02 09:36:08 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 07-02 09:36:09 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5accce0b1d749dca30712c6399e5063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-02 09:36:16 [loader.py:447] Loading weights took 7.14 seconds\n",
      "INFO 07-02 09:36:16 [gpu_model_runner.py:1186] Model loading took 2.8871 GB and 7.892670 seconds\n",
      "INFO 07-02 09:36:23 [backends.py:415] Using cache directory: /data/diepade@upvnet.upv.es/.cache/vllm/torch_compile_cache/08214e9ec6/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-02 09:36:23 [backends.py:425] Dynamo bytecode transform time: 6.55 s\n",
      "INFO 07-02 09:36:25 [backends.py:132] Cache the graph of shape None for later use\n",
      "INFO 07-02 09:36:44 [backends.py:144] Compiling a graph for general shape takes 21.03 s\n",
      "INFO 07-02 09:36:55 [monitor.py:33] torch.compile takes 27.58 s in total\n",
      "INFO 07-02 09:36:56 [kv_cache_utils.py:566] GPU KV cache size: 1,205,792 tokens\n",
      "INFO 07-02 09:36:56 [kv_cache_utils.py:569] Maximum concurrency for 32,768 tokens per request: 36.80x\n",
      "INFO 07-02 09:37:18 [gpu_model_runner.py:1534] Graph capturing finished in 22 secs, took 1.44 GiB\n",
      "INFO 07-02 09:37:18 [core.py:151] init engine (profile, create kv cache, warmup model) took 62.10 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams,AsyncLLMEngine,EngineArgs\n",
    "import os\n",
    "#del os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
    "#del os.environ['VLLM_USE_V1']\n",
    "max_seq_len = 32768\n",
    "model_family = \"Qwen\"\n",
    "model_name = \"Qwen2.5-1.5B-Instruct\"\n",
    "model_llm = f\"{model_family}/{model_name}\"\n",
    "data_type = \"auto\"\n",
    "llm = LLM(model=model_llm,gpu_memory_utilization = 0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6fac8f2-985e-45a6-92fb-cd84501ec51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a1206d6a75415abb65f4a293c44a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '6800b3d5662d5c75b69412b6',\n",
       " 'name': 'diepade',\n",
       " 'fullname': 'Diego Paracuellos de los Santos',\n",
       " 'isPro': False,\n",
       " 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ju5u9SF9Eu-GUzSb0eUrw.png',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'MetaToken',\n",
       "   'role': 'fineGrained',\n",
       "   'createdAt': '2025-04-17T08:06:47.341Z',\n",
       "   'fineGrained': {'canReadGatedRepos': False,\n",
       "    'global': [],\n",
       "    'scoped': [{'entity': {'_id': '66eaef786865fea1324edb5d',\n",
       "       'type': 'model',\n",
       "       'name': 'meta-llama/Llama-3.2-3B-Instruct'},\n",
       "      'permissions': ['repo.content.read']},\n",
       "     {'entity': {'_id': '6800b3d5662d5c75b69412b6',\n",
       "       'type': 'user',\n",
       "       'name': 'diepade'},\n",
       "      'permissions': []}]}}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login,whoami\n",
    "login()\n",
    "user = whoami(token='')\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cff0389-849c-4f13-b5c9-9e46b5c38f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ecb10b4-5774-489d-a770-e3a59633a93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0b8199-d301-4649-b5a3-19d674884d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_presidio(data,templates,eqi_table):\n",
    "    out_pulses = {}\n",
    "    types = []\n",
    "    for i in data:\n",
    "        if i.entity_type not in types:\n",
    "            types.append(i.entity_type)\n",
    "    for temp in templates:\n",
    "        if any(x in types for x in eqi_table[temp]):\n",
    "            out_pulses[temp] = 1\n",
    "        else:\n",
    "            out_pulses[temp] = 0\n",
    "    return out_pulses\n",
    "\n",
    "\n",
    "def build_messages(text,templates, out_pulse, Example):\n",
    "    text2 = []\n",
    "    for var in templates:\n",
    "        if out_pulse[var] == 1:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": (function['A'] + function[var] + function['B'] + Example[var])},\n",
    "                {\"role\": \"user\", \"content\": text}\n",
    "                \n",
    "            ]\n",
    "            text2.append(tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            ))\n",
    "    return text2\n",
    "def call_llm(messages,llm):\n",
    "    if messages != []:\n",
    "       return llm.generate(messages, sampling_params)\n",
    "    return []\n",
    "        \n",
    "def parse_output(outputs,templates,out_pulse):\n",
    "    out_temp = {}\n",
    "    i = 0\n",
    "    for ii,n in enumerate(templates):\n",
    "        if out_pulse[n] == 1:\n",
    "            try:\n",
    "                json.loads(outputs[i].outputs[0].text)\n",
    "                out_temp[n] = outputs[i].outputs[0].text\n",
    "            except:\n",
    "                out_temp[n] = Example[n]\n",
    "            i= i+1\n",
    "        else:\n",
    "            out_temp[n] = Example[n]\n",
    "    return out_temp\n",
    "def split_document(document, window_size, overlap):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    print(f\"\\tLength of document: {len(tokens)} tokens\")\n",
    "\n",
    "    chunks = []\n",
    "    if len(tokens) > window_size:\n",
    "        for i in range(0, len(tokens), window_size-overlap):\n",
    "            print(f\"\\t{i} to {i + len(tokens[i:i + window_size])}\")\n",
    "            chunk = tokenizer.convert_tokens_to_string(tokens[i:i + window_size])\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            if i + len(tokens[i:i + window_size]) >= len(tokens):\n",
    "                break\n",
    "    else:\n",
    "        chunks.append(document)\n",
    "    print(f\"\\tSplit into {len(chunks)} chunks\")\n",
    "\n",
    "    return chunks  \n",
    "def get_inference(text,templates):\n",
    "   #for t in text:\n",
    "    results = analyzer.analyze(text=text, language='en')\n",
    "    out_pulse = parse_presidio(results,templates,eqi_table)\n",
    "    #Prompt Generation\n",
    "    out_pulse = [1]\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = split_document(text,int(max_seq_len/2) , int(1024))\n",
    "    t = 0\n",
    "    output = Example\n",
    "    start = time.time()\n",
    "    for t in chunks:\n",
    "        messages = build_messages(t,templates,out_pulse,output)\n",
    "        outputs = call_llm(messages,llm)\n",
    "            # LLM calls\n",
    "        output = parse_output(outputs,templates,out_pulse) \n",
    "        \n",
    "    end= time.time()\n",
    "    t = end-start\n",
    "\n",
    "    \n",
    "    return results,output,t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f254a0d-ff16-414f-b864-58eaa2a791b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation: 443/470  ID: 3fEAJx4\n",
      "\tLength of document: 10 tokens\n",
      "\tSplit into 1 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it, est. speed input: 1029.02 toks/s, output: 119.42 toks/s]IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "outputs = []\n",
    "prompts = []\n",
    "texts = []\n",
    "templates = ['Person','Location','Contact','Identifiers','NRP','Finance','Security','Work','Others']\n",
    "\n",
    "sampling_params = SamplingParams(max_tokens = 512*18,\n",
    "                                    temperature= 0,\n",
    "                                    top_p=0.2,\n",
    "                                    skip_special_tokens = True)\n",
    "out = {}\n",
    "MyData = Valid_Data\n",
    "hh = len(MyData)\n",
    "out_conv = []\n",
    "for ii,Data in enumerate(MyData):\n",
    "    i = 0\n",
    "    jj = 0\n",
    "    text = []\n",
    "    if 'conversations' in Data:\n",
    "        out_conv  = []\n",
    "        hh = len(Data['conversations'])\n",
    "        for iii,tt in enumerate(Data['conversations']):\n",
    "            if 'gpt' not in tt['from']:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"Conversation: {iii+1}/{hh}  ID: {Data['id']}\")\n",
    "                i= i+1;\n",
    "                text_s = tt['value']\n",
    "                pres_out,part_out,t = get_inference(text_s,templates)\n",
    "                out_conv.append({'presidio':pres_out,'out':part_out, 'times':t})\n",
    "    out[Data['id']] = out_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6f3855-d343-453d-816b-15a849d1709a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uCXN7ij - 1\n",
      "nsrVEnT - 23\n",
      "XlDrXs4 - 3\n",
      "E0YL5SX - 206\n",
      "nf5GfST - 1\n",
      "64e2cii - 1\n",
      "3I3T3Xn - 15\n",
      "7mQ5Roh - 6\n",
      "5cxEGva - 75\n",
      "40HzkGG - 8\n",
      "u3J5bOB - 2\n",
      "NJDBcSe - 51\n",
      "tfnG92d - 43\n",
      "jZOf9E5 - 46\n",
      "tQUkwFZ - 4\n",
      "qmqSVor - 24\n",
      "yvK7wsp - 7\n",
      "tP2zRrR - 9\n",
      "avPN6pm - 17\n",
      "gw92JV5 - 4\n",
      "g7wipds - 11\n",
      "kjqKn2r - 6\n",
      "DhlicnH - 8\n",
      "4HUbicP - 7\n",
      "zNjGEzP - 1\n",
      "AKJuWll - 53\n",
      "XaeHl4x - 4\n",
      "uSY7o5Q - 4\n",
      "yK1UjV2 - 1\n",
      "EKEUiZP - 28\n",
      "jkXNH1N - 22\n",
      "NnnEIzG - 223\n",
      "wZx6QAq - 2\n",
      "ntuCmZn - 1\n",
      "bVmBZaM - 4\n",
      "ubZzUEa - 1\n",
      "HrWiBC7 - 1\n",
      "wrN9uUo - 179\n",
      "GrLzv0V - 287\n",
      "N6ZQok6 - 3\n",
      "nwXmG8Z - 2\n",
      "Cofs4AU - 2\n",
      "JIHAUfs - 4\n",
      "V69W6uK - 4\n",
      "nrCPDzJ - 3\n",
      "3F9YnxK - 23\n",
      "KiN4Mw2 - 2\n",
      "EDpYAcv - 1\n",
      "YFhooem - 38\n",
      "Vh95RGW - 7\n",
      "p6dMHdG - 35\n",
      "ab3MbtP - 20\n",
      "pmpZY3k - 22\n",
      "wliO4ZG - 4\n",
      "0uxvqtu - 6\n",
      "69MJdNb - 32\n",
      "WFRnBkR - 9\n",
      "LDFhSri - 50\n",
      "5kx3ba6 - 1\n",
      "f7YyRRY - 4\n",
      "8y1NTgM - 4\n",
      "beOf8Fx - 7\n",
      "IDovwuk - 17\n",
      "AJ7fUsl - 16\n",
      "N0cABRU - 4\n",
      "Caxpnd3 - 12\n",
      "0uGEftL - 3\n",
      "whwGy9S - 13\n",
      "JHr1rSz - 10\n",
      "fMcVKqE - 4\n",
      "lG7szy0 - 90\n",
      "99zadnF - 1\n",
      "pMxCH8r - 6\n",
      "GehdRG1 - 2\n",
      "x01dvhv - 2\n",
      "ls7gpmo - 20\n",
      "Y31jfyE - 9\n",
      "E5eryzD - 10\n",
      "GWefrE5 - 5\n",
      "AUbel0T - 11\n",
      "xOzzaof - 4\n",
      "hRhCVLl - 1\n",
      "oiRi6OY - 19\n",
      "DVN2lTX - 30\n",
      "6I2qtnI - 1\n",
      "Gep8xCR - 5\n",
      "gwTu0Cn - 12\n",
      "uYmbyTN - 29\n",
      "6q0SQ2N - 2\n",
      "TVvbTpP - 19\n",
      "T4PzkKo - 29\n",
      "6LMInt4 - 4\n",
      "WvRXAmQ - 27\n",
      "42aIqe1 - 9\n",
      "VD1MyV9 - 3\n",
      "rAYYUit - 12\n",
      "icaAxdT - 4\n",
      "xyZ2N48 - 17\n",
      "UQSxZjs - 8\n",
      "64V8QV0 - 4\n",
      "SweKUpR - 1\n",
      "1weZDP7 - 18\n",
      "6Ekh9sE - 26\n",
      "Shh06zM - 46\n",
      "mrZ90jA - 156\n",
      "7ia22C1 - 2\n",
      "j4hlhF5 - 38\n",
      "RYlSE0T - 3\n",
      "kTtcsIZ - 28\n",
      "P4dXQzo - 31\n",
      "Go6eYAt - 3\n",
      "y1M2Cjf - 5\n",
      "Qq7mXzq - 28\n",
      "j91l0RS - 10\n",
      "S7LpwOO - 104\n",
      "RwUQP9w - 7\n",
      "qUdqloP - 16\n",
      "rc0nx1Q - 20\n",
      "EjLUVeT - 13\n",
      "Nty2sek - 29\n",
      "btnZsqt - 28\n",
      "1ijOBu9 - 457\n",
      "bnjDd43 - 5\n",
      "NpdY6CT - 11\n",
      "6MiKJZ9 - 20\n",
      "8dh1NK0 - 11\n",
      "RmMXauf - 27\n",
      "mZj9xGN - 5\n",
      "3fEAJx4 - 235\n",
      "qNuE6K3 - 8\n",
      "8LNoWNd - 19\n",
      "kS7P3dA - 1\n",
      "lEmm4Wg - 4\n",
      "NqXFUQn - 4\n",
      "ynYgOeD - 92\n",
      "gh0Yquq - 24\n",
      "a3IaNpM - 2\n",
      "f9Y3Wf3 - 12\n",
      "UFEY6om - 23\n",
      "79B1qmI - 17\n",
      "appvvxq - 4\n",
      "BEIbEA9 - 12\n",
      "46BcenZ - 2\n",
      "CX6wboP - 16\n",
      "1WB6Vnl - 156\n",
      "YkWoiab - 17\n",
      "s6VZ0KZ - 5\n",
      "eb73Dvd - 4\n",
      "W0mpVwq - 1\n",
      "b0QlRXl - 28\n",
      "PWvkXC9 - 1\n",
      "AOSCySg - 3\n",
      "EKAA5ny - 8\n",
      "1X4rDaC - 26\n",
      "l2YXBj7 - 4\n",
      "o9gad84 - 17\n",
      "c8sWah6 - 8\n"
     ]
    }
   ],
   "source": [
    "Valid_Data2 = {}\n",
    "\n",
    "for vd in Valid_Data:\n",
    "    ida = vd['id']\n",
    "    hg = []\n",
    "    for vv in vd['conversations']:\n",
    "        if vv['from'] == 'gpt':\n",
    "            continue\n",
    "        hg.append(vv)\n",
    "    Valid_Data2[ida] = hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f84c852-48db-4e28-a82b-56030f268a9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US_DRIVER_LICENSE\n",
      "0.3\n",
      "US Consulate General\n",
      "C49, G Block Rd, G Block BKC, Bandra Kurla Complex, Bandra East, Mumbai, Maharashtra 400051, India\n",
      "Dear Consul General,\n",
      "February 6th, 2023\n",
      "North America Telugu Society (NATS) is one of the largest non-profit national organizations representing Telugu speaking people of North America. NATS primary objective is to address the issues and concerns that affect the everyday lives of Telugu people living in North America. America Telugu Sambaralu (NATS Biennial National conference), is being held at NJ Expo Center, 97 Sun Field Ave, Edison, NJ from May 26th to May 28th 2023. This three-day event is packed with a variety of activities including star-studded cultural programs, literary and spiritual activities, CME sessions, Zellenial (Next generation group) programs and activities, alumni meetings, health forums, Women’s forum, , motivational speeches and business seminars. This conference serves the purpose of providing a platform to exchange ideas, interact with each other, and celebrate a rich cultural heritage along with NATS basic principle of ‘SERVING THE COMMUNITY’. More information about our conference can be found at http://www.sambaralu.org/.\n",
      "We are expecting around 8,000 to 10,000 people to attend our conference. Our budget for the conference is about USD 2.5 million. As any charitable organization, we solely depend on donors and sponsors to successfully conduct an event of this magnitude. Vega Sri Gold & Diamonds will be one of our event sponsors. NATS is pleased to extend our invitation to two partners of Vega Sri listed below to attend America Telugu Sambaralu being held at NJ Expo Center, 97 Sun Field Ave, Edison, NJ from May 26th to May 28th 2023. This gives them an opportunity to explore and expand their business in United States.\n",
      "1. YECHURI MANIDEEP, Founder and Managing Partner (Passport# W4019739)\n",
      "2. GOLLA KALYAN KUMAR, Partner (Passport# N0751298)\n",
      "We sincerely request you to issue B1/B2 Visa to above individuals so they can attend our event as sponsors. As part of their sponsorship, NATS will provide hotel accommodation and food arrangements during the event. If you need any additional information, please feel free to contact me @610 906 5208 or email: convenor@sambaralu.org.\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "out2 = {}\n",
    "for o in out:\n",
    "    k = []\n",
    "    \n",
    "    for a,l in enumerate(out[o]):\n",
    "        v = {}\n",
    "        v['out'] = l['out']\n",
    "        v['times'] = l['times']\n",
    "        data = l['presidio']\n",
    "        nd = []\n",
    "        for i,d in enumerate(data):\n",
    "            jd = {}\n",
    "            jd['type'] = d.entity_type \n",
    "            jd['value'] = Valid_Data2[o][a]['value'][d.start:d.end]\n",
    "            jd['score'] = d.score\n",
    "            nd.append(jd)\n",
    "        v['presidio'] = nd\n",
    "        k.append(v)\n",
    "    out2[o] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eec5ef2-021e-45dd-8aa9-32029087a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn = 0\n",
    "import datetime \n",
    "cur_time = datetime.date.today().strftime(\"%H_%M_%B_%d_%Y\")\n",
    "output_file = f\"./outputs/vllm_test_single_out_date_{cur_time}_{model_name}_{data_type}_{nn}_v1.json\"\n",
    "out_parsed = []\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(out2, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vLLM]",
   "language": "python",
   "name": "conda-env-vLLM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

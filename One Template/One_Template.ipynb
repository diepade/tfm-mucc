{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6bea8e-a003-41d3-b127-53e78143e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function = {}\n",
    "function['A'] = \"\"\"You are an extraction model designed to identify and extract Personally Identifiable Information (PII) from text and format it into a structured JSON. Your task is to analyze the provided text and extract the relevant data fields as specified below. \n",
    "\n",
    "Ensure that the output adheres to the exact JSON structure, including all fields, even if they are empty. If no data is found, output an empty JSON with the same structure.\n",
    "\n",
    "\"\"\"\n",
    "function['B'] = \"\"\"\n",
    "Please provide only the JSON output, with no additional comments or explanations. Use the following format:\n",
    "\"\"\"\n",
    "template = \"\"\"\n",
    "{\n",
    "\t\"Personal_Information\": {\n",
    "\t\t\"Person\": {\n",
    "\t\t\t\"Name\": \"\",\n",
    "\t\t\t\"National_ID\": \"\",\n",
    "\t\t\t\"Passport_Number\": \"\",\n",
    "\t\t\t\"Social_Security_Number\": \"\",\n",
    "\t\t\t\"Birth_Date\": \"\",\n",
    "\t\t\t\"Age\": \"\",\n",
    "\t\t\t\"Height\": \"\",\n",
    "\t\t\t\"Weight\": \"\",\n",
    "\t\t\t\"Gender\": \"\",\n",
    "\t\t\t\"Marital_Status\": \"\",\n",
    "\t\t\t\"Number_of_Children\": \"\",\n",
    "\t\t\t\"Nationality_Citizenship\": \"\",\n",
    "\t\t\t\"Place_of_Birth\": \"\",\n",
    "\t\t\t\"Mother's_Maiden_Name\": \"\",\n",
    "\t\t\t\"Race_Ethnic\": \"\",\n",
    "\t\t\t\"Religion\": \"\",\n",
    "\t\t\t\"Philosophical_Belief\": \"\",\n",
    "\t\t\t\"Political_Affiliation\": \"\",\n",
    "\t\t\t\"Trade_Union_Affiliation\": \"\",\n",
    "\t\t\t\"Sexual_Preference\": \"\",\n",
    "\t\t\t\"Sex_Life\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Appearance\": {\n",
    "\t\t\t\"Picture_of_Face\": \"\",\n",
    "\t\t\t\"Distinguishing_Characteristic\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Contact_Information\": {\n",
    "\t\t\t\"Home_Address\": {\n",
    "\t\t\t\t\"Street_Address\": \"\",\n",
    "\t\t\t\t\"City\": \"\",\n",
    "\t\t\t\t\"State\": \"\",\n",
    "\t\t\t\t\"ZIP_Code\": \"\",\n",
    "\t\t\t\t\"Country\": \"\"\n",
    "\t\t\t},\n",
    "\t\t\t\"Phone_Number\": \"\",\n",
    "\t\t\t\"Email_Address\": \"\",\n",
    "\t\t\t\"Family_Friend_Contact_Information\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Online_Identifiers\": {\n",
    "\t\t\t\"Screen_Name\": \"\",\n",
    "\t\t\t\"Social_Network_Profile\": \"\",\n",
    "\t\t\t\"Social_Network_Activity\": \"\",\n",
    "\t\t\t\"URLs\": \"\",\n",
    "\t\t\t\"Online_Identifiers\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Location_Information\": {\n",
    "\t\t\t\"Home_Town_City\": \"\",\n",
    "\t\t\t\"Geographical_Indicators\": \"\",\n",
    "\t\t\t\"Geo_Location\": \"\",\n",
    "\t\t\t\"Country\": \"\",\n",
    "\t\t\t\"ZIP_Code\": \"\",\n",
    "\t\t\t\"Address\": \"\",\n",
    "\t\t\t\"Date_Time\": \"\"\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"Work_Information\": {\n",
    "\t\t\"Job_Title\": \"\",\n",
    "\t\t\"Occupation\": \"\",\n",
    "\t\t\"Work_ID\": \"\",\n",
    "\t\t\"Work_Address\": {\n",
    "\t\t\t\"Street_Address\": \"\",\n",
    "\t\t\t\"City\": \"\",\n",
    "\t\t\t\"State\": \"\",\n",
    "\t\t\t\"ZIP_Code\": \"\",\n",
    "\t\t\t\"Country\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Work_Contact_Information\": {\n",
    "\t\t\t\"Work_Phone_Number\": \"\",\n",
    "\t\t\t\"Work_Email_Address\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Employment_Information\": {\n",
    "\t\t\t\"Employment_Status\": \"\",\n",
    "\t\t\t\"Work_Experience\": \"\",\n",
    "\t\t\t\"Skills\": \"\",\n",
    "\t\t\t\"Education\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Income_Level\": \"\"\n",
    "\t},\n",
    "\t\"Financial_Information\": {\n",
    "\t\t\"Banking_Details\": {\n",
    "\t\t\t\"Credit_Card_Number\": \"\",\n",
    "\t\t\t\"Credit_Score\": \"\",\n",
    "\t\t\t\"ABA_Routing_Number\": \"\",\n",
    "\t\t\t\"Bank_Account_Number\": \"\",\n",
    "\t\t\t\"Individual_Taxpayer_Identification\": \"\",\n",
    "\t\t\t\"SWIFT_Code\": \"\",\n",
    "\t\t\t\"Crypto\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Invoice_Payments\": \"\",\n",
    "\t\t\"Financial_Information\": \"\"\n",
    "\t},\n",
    "\t\"Security_Information\": {\n",
    "\t\t\"Digital_Signature\": \"\",\n",
    "\t\t\"Password\": \"\",\n",
    "\t\t\"License_Numbers\": {\n",
    "\t\t\t\"Drivers_License_Number\": \"\",\n",
    "\t\t\t\"Vehicle_Registration_Number\": \"\",\n",
    "\t\t\t\"License_Plate_Number\": \"\"\n",
    "\t\t},\n",
    "\t\t\"Biometric_Data\": {\n",
    "\t\t\t\"Fingerprint_Data\": \"\",\n",
    "\t\t\t\"Voice_Print\": \"\",\n",
    "\t\t\t\"Handwriting_Sample\": \"\",\n",
    "\t\t\t\"Physiological_Data\": \"\",\n",
    "\t\t\t\"Genetic_Data\": \"\",\n",
    "\t\t\t\"X_Ray\": \"\",\n",
    "\t\t\t\"Biometric_Data\": \"\"\n",
    "\t\t}\n",
    "\t},\n",
    "\t\"Health_Information\": {\n",
    "\t\t\"Health_Insurance_ID\": \"\",\n",
    "\t\t\"Medical_History\": \"\",\n",
    "\t\t\"Physiological_Data\": \"\"\n",
    "\t},\n",
    "\t\"Cultural_and_Social_Identity\": {\n",
    "\t\t\"Cultural_Social_Identity\": \"\",\n",
    "\t\t\"Shopping_Behavior\": \"\",\n",
    "\t\t\"Survey_Answers\": \"\",\n",
    "\t\t\"Signed_Petitions\": \"\",\n",
    "\t\t\"Activities\": \"\",\n",
    "\t\t\"Law_Enforcement_Records\": \"\"\n",
    "\t}\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ea459-2221-4d41-bf82-1d072bf7fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import re\n",
    "\n",
    "label_name = \"./ShareGPT sample (N=200) - Sheet1.csv\"\n",
    "save_file = 'test.json'\n",
    "\n",
    "dataset_name = \"./sg_90k_part1.json\"\n",
    "dataset_name2 = \"./sg_90k_part2.json\"\n",
    "# Open and read the JSON file\n",
    "with open(dataset_name, 'r') as file:\n",
    "    data_1 = json.load(file)\n",
    "\n",
    "with open(dataset_name2, 'r') as file:\n",
    "    data_2 = json.load(file)\n",
    "\n",
    "data_1 = data_1 + data_2\n",
    "data_2 = []\n",
    "\n",
    "IDs = []\n",
    "PIIs = []\n",
    "with open(label_name, newline='') as csvfile:\n",
    "    spamreader = csv.DictReader(csvfile)\n",
    "    for row in spamreader:\n",
    "            IDs.append(row['Chat ID'])\n",
    "            PIIs.append(row['PII types'])\n",
    "ii = 0\n",
    "Valid_Data = []\n",
    "dataset_classes = []\n",
    "for dat in data_1:\n",
    "    k = [0]* 8\n",
    "    if dat['id'] not in IDs:\n",
    "        ii = ii+1\n",
    "        continue\n",
    "    idx = IDs.index(dat['id'])\n",
    "    PII = PIIs[idx]\n",
    "    \n",
    "    k[0] = 1 if 'DATE_TIME' in PII else 0\n",
    "    k[1] = 1 if 'EMAIL_ADDRESS' in PII else 0\n",
    "    k[2] = 1 if 'LOCATION' in PII else 0\n",
    "    k[3] = 1 if 'NRP' in PII else 0\n",
    "    k[4] = 1 if 'PASSPORT_NUMBER' in PII else 0\n",
    "    k[5] = 1 if 'PERSON' in PII else 0\n",
    "    k[6] = 1 if 'PHONE_NUMBER' in PII else 0\n",
    "    k[7] = 1 if 'URL' in PII else 0\n",
    "    #print(f\"{k} - {idx} - {dat['id']} - {IDs[idx]}\")\n",
    "    Valid_Data.append({'id':IDs[idx],\n",
    "                       'PII':k,\n",
    "                       'conversations':dat['conversations']})\n",
    "\n",
    "data_1 = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530a93f-8f52-4570-9694-e848e065eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams,AsyncLLMEngine,EngineArgs\n",
    "from vllm import TokensPrompt\n",
    "MAX_INPUT_SIZE = 8_192\n",
    "MAX_NEW_TOKENS = 6000\n",
    "\n",
    "sampling_p = SamplingParams(truncate_prompt_tokens = MAX_INPUT_SIZE,temperature= 1.0,top_p = 1,min_p = 0,top_k = 50,skip_special_tokens = True,max_tokens = MAX_NEW_TOKENS)\n",
    "\n",
    "llm = LLM(model=\"numind/NuExtract-1.5-smol\",\n",
    "          gpu_memory_utilization = 0.95,\n",
    "          dtype='bfloat16',\n",
    "          max_model_len = MAX_INPUT_SIZE,\n",
    "          max_num_seqs = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b8199-d301-4649-b5a3-19d674884d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_json_text(text):\n",
    "    text = text.strip()\n",
    "    text = text.replace(\"\\\\#\",\"#\").replace(\"\\\\&\",\"&\")\n",
    "    return text\n",
    "\n",
    "def predict_chunk(text, template,current,model, tokenizer):\n",
    "    current = clean_json_text(current)\n",
    "    input_llm = f\"<|input|>\\n### Template:\\n{template}\\n### Current:\\n{current}\\n### Text:\\n{text}\\n\\n<|output|>\"+ \"{\"\n",
    "    #print(input_ids)\n",
    "    output = model.generate(input_llm,sampling_p)\n",
    "\n",
    "    output = tokenizer.decode(output[0].outputs[0].token_ids,skip_special_tokens=True)\n",
    "\n",
    "    return clean_json_text(output.split(\"<|output|>\")[0])\n",
    "def split_document(document, window_size, overlap,tokenizer):\n",
    "    tokens = tokenizer.tokenize(document)\n",
    "    print(f\"\\tLength of document: {len(tokens)} tokens\")\n",
    "\n",
    "    chunks = []\n",
    "    if len(tokens) > window_size:\n",
    "        for i in range(0, len(tokens), window_size-overlap):\n",
    "            print(f\"\\t{i} to {i + len(tokens[i:i + window_size])}\")\n",
    "            chunk = tokenizer.convert_tokens_to_string(tokens[i:i + window_size])\n",
    "            chunks.append(chunk)\n",
    "\n",
    "            if i + len(tokens[i:i + window_size]) >= len(tokens):\n",
    "                break\n",
    "    else:\n",
    "        chunks.append(document)\n",
    "    print(f\"\\tSplit into {len(chunks)} chunks\")\n",
    "\n",
    "    return chunks\n",
    "    \n",
    "def check_structure(dic1,dic2):\n",
    "    if dic1.keys() != dic2.keys():\n",
    "        print(f\"{dic1.keys()} is not {dic2.keys()}\")\n",
    "        return False\n",
    "    for key in dic1:\n",
    "        if isinstance(dic1[key], dict) and isinstance(dic2[key], dict):\n",
    "            if not check_structure(dic1[key], dic2[key]):\n",
    "                return False\n",
    "        elif not isinstance(dic1[key], dict) and not isinstance(dic2[key], dict):\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"{dic1[key]} is not {dic2[key]}\" )\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def handle_broken_output(pred, prev):\n",
    "    if (pred[0] != \"{\"):\n",
    "        pred = \"\\n{\\n\\t\" + pred\n",
    "    try:\n",
    "        if not check_structure(dict(json.loads(prev)),dict(json.loads(pred))):\n",
    "            print(\"structures does not match\")\n",
    "            pred = prev\n",
    "            return pred\n",
    "            \n",
    "    except:\n",
    "        print(\"Data Malformation detected.\")\n",
    "        #print(pred)\n",
    "        pred = prev\n",
    "        return pred\n",
    "    print(\"matching structures and new data acquired.\")\n",
    "    return pred\n",
    "\n",
    "def sliding_window_prediction(text, template, model, window_size=4000, overlap=128):\n",
    "    # split text into chunks of n tokens\n",
    "    tokenizer = model.get_tokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = split_document(text, window_size, overlap,tokenizer)\n",
    "    s = time.time()\n",
    "    # iterate over text chunks\n",
    "    prev = template\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i}...\")\n",
    "        pred = predict_chunk(chunk, template, prev, model, tokenizer)\n",
    "\n",
    "        # handle broken output\n",
    "        pred = handle_broken_output(pred, prev)\n",
    "            \n",
    "        # iterate\n",
    "        prev = pred\n",
    "    e = time.time()\n",
    "    t = e - s\n",
    "    return pred,t\n",
    "def call_llm(llm, text, template , sampling_p):\n",
    "    sub_texts = text_splitter.split_text(text)\n",
    "    parts = len(sub_texts)\n",
    "    if parts > 1:\n",
    "        print(f\"Split text in {parts} parts\")\n",
    "    t = 0 \n",
    "    #Prompt Generation\n",
    "    prompts_PIIs = [f\"\"\"<|input|>\\n### Template:\\n{template}\\n### Current:\\n{tt} Text:\\n{text}\\n\\n<|output|>\\n\"\"\" for tt in sub_texts]\n",
    "    start = time.time()\n",
    "    out = llm.generate(prompts_PIIs,sampling_p)\n",
    "    end= time.time()\n",
    "    t = end-start\n",
    "        \n",
    "    return out,t,parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f254a0d-ff16-414f-b864-58eaa2a791b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "from vllm.sampling_params import GuidedDecodingParams\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "import vllm\n",
    "outputs = []\n",
    "prompts = []\n",
    "texts = []\n",
    "size = 4000\n",
    "overlap = 128\n",
    "text_splitter = TokenTextSplitter(chunk_size=size , chunk_overlap=overlap)\n",
    "out = []\n",
    "MyData = Valid_Data\n",
    "hh = len(MyData)\n",
    "out_conv = []\n",
    "for ii,Data in enumerate(MyData):\n",
    "    i = 0\n",
    "    jj = 0\n",
    "    if 'conversations' in Data:\n",
    "        for n in Data['conversations']:\n",
    "             if 'gpt' not in n['from']:\n",
    "                 jj=jj+1\n",
    "        out_text = []\n",
    "        for tt in Data['conversations']:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Conversation: {ii+1}/{hh} - Prompt: {i+1}/{jj} ID: {Data['id']}\")\n",
    "            if 'gpt' not in tt['from']:\n",
    "                i= i+1;\n",
    "                text = tt['value']\n",
    "                part_out,t = sliding_window_prediction(text, template, llm, window_size=4000, overlap=128)\n",
    "                out_text.append({'out':part_out, 'times':t})\n",
    "        out_conv.append({\"id\":Data['id'],\"data\":out_text})\n",
    "        \n",
    "    else:\n",
    "        text = Data\n",
    "        part_out,t = call_llm(llm, analyzer, text, templates , sampling_p)\n",
    "        out_text.append({'out':part_out, 'times':t})\n",
    "        out_conv.append(out_text)\n",
    "    out.append(out_conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4354817f-b595-46b2-b8d7-051275af96b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec5ef2-021e-45dd-8aa9-32029087a271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nn = 0\n",
    "import datetime \n",
    "cur_time = datetime.date.today().strftime(\"%H_%M_%B_%d_%Y\")\n",
    "output_file = f\"./outputs/t_vllm_tiny_test_single_template_single_out_date_{cur_time}_n_{nn}_6.json\"\n",
    "out_parsed = []\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(out_conv, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vLLM]",
   "language": "python",
   "name": "conda-env-vLLM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
